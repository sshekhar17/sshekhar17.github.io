[{"authors":["admin"],"categories":null,"content":"I am a Ph.D. candidate in the ECE Department at University of California, San Diego advised by Prof. Tara Javidi. My research interests lie in the general areas of maching learning and sequential decision making. In particular, I have worked on the topics of Bayesian Optimization, Active Learning and Reinforcement Learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://sshekhar17.githuo.io/author/shubhanshu-shekhar/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/shubhanshu-shekhar/","section":"authors","summary":"I am a Ph.D. candidate in the ECE Department at University of California, San Diego advised by Prof. Tara Javidi. My research interests lie in the general areas of maching learning and sequential decision making.","tags":null,"title":"Shubhanshu Shekhar","type":"authors"},{"authors":[],"categories":[],"content":"Summary: We introduce LeCam\u0026rsquo;s method for obtaining minimax lower bounds for statistical estimation problems, which proceeds by relating the probabililty of error of a binary hypothesis testing problem to the total-variation distance between the two distributions. Then, as a novel application, we use this technique to derive lower bound on the problem of active learning in bandits which shows the near optimality of the existing algorithms due to Antos et al. (2008) and Carpentier et al. (2011). \n Consider the following setting:\n Suppose $\\mathcal{P}$ denotes a class of probability distributions, $(\\mathcal{D}, d)$ is a metric space1, $\\theta: \\mathcal{P} \\mapsto \\mathcal{D}$ represents some $\\mathcal{D}$-valued functional, $\\mathcal{D}_1, \\mathcal{D}_2$ represent two disjoint and $2\\delta$ separated subsets of $\\mathcal{D}$, for some $\\delta\u0026gt;0$, $\\mathcal{P}_i = \\theta^{-1}(\\mathcal{D}_i )$ for $i=1,2$ are non-empty disjoint subsets of $\\mathcal{P}$.  Suppose we are given a sample $X \\sim P$ taking values in some set $\\mathcal{X}$ for $P \\in \\mathcal{P}$, and let $\\hat{\\theta}: \\mathcal{X} \\mapsto \\mathcal{D}$ denote an estimator of $\\theta(P)$. Then for any estimator $\\hat{\\theta}$ we can obtain the following lower bound on the maximum expected error:\n Lemma 1 (Le Cam). With the definitions introduced above, we have $$ \\sup_{P \\in \\mathcal{P}} \\mathbb{E}_P \\left[ d \\left( \\hat{\\theta}, , \\theta(P) \\right) \\right] \\geq \\delta \\max_{P_i \\in \\mathcal{P}_i} \\left(1 - d_{TV}\\left(P_1, P_2 \\right) \\right). $$\n In the above display, $d_{TV}$ denotes the total variation distance between two distributions, defined as $d_{TV}(P, Q) = \\sup_{E } P(E) - Q(E)$ where the supremum is over all measurable sets $E$. The result above implies that the minmax lower bound for a particular estimation problem is large if there exist two distributions $P_1$ and $P_2$ which (i) are \u0026lsquo;well-separated\u0026rsquo; in terms of the $d-$metric, and (ii) are statistically \u0026lsquo;close\u0026rsquo; in terms of $d_{TV}(\\cdot, \\cdot)$. Due to their opposing nature, obtaining the best lower bound requires finding the right balance between these two requirements.\nThe statement of the above result and its proof are based on the statement and proof of Lemma~1 in (Yu, 1997)2.\nProof of the Lemma First, we select arbitrary $P_i \\in \\mathcal{P}_i$ for $i=1,2$ and lower-bound the supremum over all $P \\in \\mathcal{P}$ with a simple average over these two distributions.\n$$ \\sup_{P \\in \\mathcal{P}}\\;\\mathbb{E}_P \\left[ d \\left( \\hat{\\theta}, , \\theta(P) \\right) \\right]\n\\geq \\frac{1}{2} \\left( \\mathbb{E}_{P_1} \\left[ d \\left( \\hat{\\theta}, , \\theta(P_1) \\right) \\right] + \\mathbb{E}_{P_2} \\left[ d \\left( \\hat{\\theta}, , \\theta(P_2) \\right) \\right]\\right ) $$ Next, with the notation $\\theta_i = \\theta(P_i)$, we observe3 the following: for any $\\theta \\in \\mathcal{D}$ such that $d(\\theta, \\theta_1)\u0026lt; \\delta$, we must have $d(\\theta, \\theta_2)\u0026gt;\\delta$. Similarly, if $d(\\theta, \\theta_2)\u0026lt;\\delta$ then $d(\\theta, \\theta_1)\u0026gt;\\delta$. Both these results follow from the fact that $d$ satisfies the triangle inequality and that $d(\\theta_1, \\theta_2) \\geq 2\\delta$. Now, we define the set $E = \\left \\{x \\in \\mathcal{X}\\;:\\; d\\left( \\hat{\\theta}(x), \\theta_1 \\right) \u0026lt; d\\left( \\hat{\\theta}(x), \\theta_2 \\right) \\right \\}$. Clearly, if $X \\in E$, then $d(\\hat{\\theta}(X), \\theta_2) \\geq \\delta$ and if $X \\in E^c$ then $d(\\hat{\\theta}, \\theta_1) \\geq \\delta$. Together, these results imply that $d(\\hat{\\theta}, \\theta_1) \\geq \\delta 1_{E^c}$ and $d(\\hat{\\theta}, \\theta_2) \\geq \\delta 1_E$ where $1_E$ denotes the indicator function associated with a set $E$. Thus we have $$ \\begin{aligned} \\sup_{P \\in \\mathcal{P}}\\;\\mathbb{E}_P \\left[ d \\left( \\hat{\\theta}, \\, \\theta(P) \\right) \\right] \u0026amp; \\geq \\frac{1}{2} \\left( \\mathbb{E}_{P_1} \\left[ d \\left( \\hat{\\theta}, \\, \\theta(P_1) \\right) \\right] + \\mathbb{E}_{P_2} \\left[ d \\left( \\hat{\\theta}, \\, \\theta(P_2) \\right) \\right]\\right ) \\\\\n\u0026amp; \\geq \\frac{1}{2} \\left( \\mathbb{E}_{P_1}[\\delta 1_{E^c} ] + \\mathbb{E}_{P_2}[\\delta 1_{E}]\\right) \\; = \\; \\frac{\\delta}{2} \\left( P_1(E^c) + P_2(E) \\right) \\\\\n\u0026amp; \\geq \\frac{\\delta}{2} \\left( 1 - \\left(P_1(E) - P_2(E) \\right) \\right) \\\\\n\u0026amp; \\geq \\frac{\\delta}{2} \\left( 1 - \\sup_{E \\subset \\mathcal{X}}\\left(P_1(E) - P_2(E) \\right) \\right) \\\\\n\u0026amp; = \\frac{\\delta}{2} \\left( 1 - d_{TV}(P_1, P_2) \\right) \\\\\n\\end{aligned} $$\nFinally, the result follows by noting the fact that the distributions $P_i \\in \\mathcal{P}_i$ were chosen arbitrarily, and hence we can take a supremum over all such $P_i$.\n Application to Active Learning in Bandits. We first describe the problem of active learning in $K$-armed bandits. A multi-armed bandit (MAB) problem~(with $K$ arms) consists of $K$ distributions $(P_1, \\ldots, P_K)$ which can be individually sampled by an agent. In the problem of active learning in bandits, given a total sampling budget of $n$, the goal of an agent is to allocate these $n$ samples among these $K$ distributions, in order to learn their means uniformly well. More specifically, suppose the agent allocates $T_i \\geq 1$ samples to distribution $P_i$, with $\\sum_{i=1}^K T_i = n$ and constructs the empirical estimate of the mean of $P_i$, $\\hat{\\mu}_i(T_i) = \\frac{1}{T_i}\\sum_{j=1}^{T_i}X_{i,j}$. Then the goal is to find the allocation $(T_1, \\ldots, T_K)$ which solves\n$$ \\begin{aligned} \\min_{T_1, \\ldots, T_K: \\sum_{i=1}^K T_i = n} \\;\\max_{1 \\leq i \\leq K} \\; \\mathbb{E} \\left[ |\\hat{\\mu_i}(T_i) - \\mu_i|^2\\right] = \\left( \\max_{1 \\leq i \\leq K}\\frac{ \\sigma_i^2}{T_i} \\right ) \\stackrel{\\text{def}}{=}\\mathcal{L}(T_1, \\ldots, T_K) \\end{aligned} $$ Allowing, for $T_i$ to take real values, the optimal allocation for this above problem is given by $T_i^* = \\frac{ \\sigma_i^2 n}{\\Sigma}$ where $\\Sigma = \\sum_{i=1}^K \\sigma_i^2$. Clearly, the optimal allocation $(T_1^*, \\ldots, T_K^*)$ depends on the variance of the distributions $(P_i)_{i=1}^K$ which are unknown to the agent, and the task\nis to design an adaptive sampling strategy which appropriately addresses this explore-exploit dilemma and ends up with an allocation $(T_1, \\ldots, T_K)$ which is close to the optimal.\nLower bound construction Consider two Bernoulli distributions $U \\sim \\text{Ber(u)}$ and $V \\sim \\text{Ber}(v)$ with $1/2 \u0026lt; v \u0026lt; u \u0026lt;1$ and let $\\mathcal{M} = (U, V)$ and $\\mathcal{N} = (V, U)$ be two two-armed bandit problems. Suppose an allocation scheme $\\mathcal{A}$ is applied on one of these two problems and results in an allocation $(T_1, T_2)$. Then we have the following result, which informally says that for $u, v$ close enough, no algorithm can perform well on both the problems.\n Proposition 2. We have the following: $$ \\inf_{\\mathcal{A}}\\; \\max_{\\mathcal{M}, \\mathcal{N}}\\; \\max_{i =1, 2}\\; \\mathbb{E}\\left[ |T_i - T_i^*| \\right] = \\Omega (\\sqrt{n}). $$\n Proof of Proposition 2. Together with the MAB instance (i.e., either $\\mathcal{M}$ or $\\mathcal{N}$), the allocation scheme $\\mathcal{A}$ induces a probability distribution on the space of action-observation sequences $(a_1, X_1, a_2, X_2, \\ldots, a_n, X_n)$ where each action $a_t \\in \\{1,2\\}$ and the observations $X_t$ lie in $\\{0, 1\\}$ since the distributions $U$ and $V$ are Bernoulli. We will denote the two resulting probability distributions by $P_1$ and $P_2$, corresponding to MABs $\\mathcal{M}$ and $\\mathcal{N}$ respectively.\nWe also introduce the notations $T_U^* = \\frac{\\sigma_U^2 n}{\\sigma_U^2 + \\sigma_V^2}$ and $T_V^* = \\frac{\\sigma_V^2 n}{\\sigma_U^2 + \\sigma_V^2}$. Then under the MAB $\\mathcal{M}$ the optimal allocations are $(T_1^*, T_2^*) =(T_U^*, T_V^*)$, while they are flipped for the MAB $\\mathcal{N}$. Since we have assumed that $u\u0026gt;v$, we must have $\\sigma_U^2 \u0026lt; \\sigma_V^2$ which implies that $T_U^* \u0026lt; n/2 \u0026lt; T_V^*$.\nTo apply Lemma~1 to this problem, we introduce the following notations keeping the algorithm $\\mathcal{A}$ fixed.\n  We choose $\\mathcal{P}$ to be the set $\\{P_1, P_2\\}$ where $P_i$ for $i\\in \\{1,2\\}$ were defined earlier. Since there are only two elements in $\\mathcal{P}$, we trivially have $\\mathcal{P}_i = \\{P_i\\}$ for $i \\in \\{1,2\\}$.\n  We define $\\theta:\\mathcal{P} \\mapsto \\mathbb{R}$ as as the mapping from $P \\in \\mathcal{P}$ to the corresponding $T_1^* $. That is, $\\theta(P_1) = T_U^* $ and $\\theta(P_2) = T_V^* $. The metric $d$ is chosen as $d(t_1, t_2) = |t_1 - t_2|$.\n  The estimate $\\hat{\\theta}$ is the allocation value $T_1$ resulting from the scheme $\\mathcal{A}$. Note that since $T_1 + T_2 = n$, we have $|T_1^* - T_1| = |T_2^* - T_2|$. Therefore, we always have $\\mathbb{E} [ |T_1-T_1^*| ] = \\max_{i = 1,2} \\mathbb{E} [|T_i - T_i^*|]$.\n  Finally, we introduce the notation $\\delta = |T_U^* - T_V^*|/2$.\n  Within this setting, a direct application of Lemma~1 gives us $$ \\max_{\\mathcal{M}, \\mathcal{N}} \\; \\max_{i=1,2} \\; \\mathbb{E}[|T_i^*-T_i|] \\geq \\delta \\left( 1- d_{TV}(P_1, P_2) \\right). \\qquad (\\star) $$\nNext, we need to obtain a lower bound on $\\delta$ and an upper bound on $d_{TV}(P_1, P_2)$.\nLower bound on $\\delta$: First we note that $\\sigma_U^2 = u(1-u)$ and $\\sigma_V^2 = v(1-v)$ and $2\\delta = n \\frac{\\sigma_V^2 - \\sigma_U^2}{\\sigma_V^2 + \\sigma_U^2}$. With the notation $v = u-\\epsilon$ and the fact that $1/2 \u0026lt; v \u0026lt; u \u0026lt; 1$, we can show that $\\delta \\geq n (u-1/2)\\epsilon$. By choosing $u=3/4$, we get $\\delta \\geq \\frac{n\\epsilon}{4}$.\nUpper bound on $(d_{TV}(P_1, P_2)):$ To bound $d_{TV}(P_1, P_2)$ we proceed in the following steps: $$ \\begin{aligned} d_{TV}(P_1, P_2) \u0026amp; \\stackrel{(i)}{\\leq} \\sqrt{\\frac{D_{KL}(P_1, P_2)}{2}} \\stackrel{(ii)}{=} \\sqrt{ \\frac{ \\mathbb{E}[T_1]d_{KL}(u,v) + \\mathbb{E}[T_2]d_{kl}(v,u)}{2} } \\\\\n\u0026amp; \\stackrel{(iii)}{\\leq } 4(u-v) \\sqrt{ \\frac{ \\mathbb{E}[T_1] + \\mathbb{E}[T_2] }{6} } = \\epsilon \\sqrt{\\frac{8n}{3} } \\end{aligned} $$ In the above display,\n (i) follows from an application of Pinsker\u0026rsquo;s inequality, (ii) follows from the decomposition lemma for KL-divergence for bandits (see Eq.(5) here), and (iii) uses the fact that the bound on KL-divergence for Bernoulli random variables $d_{KL}(u, v) \\leq \\frac{(u-v)^2}{v(1-v)}$.  Thus plugging these two inequalities back into the inequality $(\\star)$, and choosing $\\epsilon = \\sqrt{ \\frac{3}{32n} }$ gives us $$ \\begin{aligned} \\max_{\\mathcal{M}, \\mathcal{N}} \\; \\max_{i=1,2} \\; \\mathbb{E}[|T_i^*-T_i|] \u0026amp;\\geq \\frac{n \\epsilon}{4} \\left(1 - \\epsilon \\sqrt{\\frac{8n}{3}} \\right) \\\\\n\u0026amp; = \\frac{1}{32} \\sqrt{ \\frac{3n}{2} } = \\Omega ( \\sqrt{n} ) \\end{aligned} $$\n$\\blacksquare$  Note that this $\\Omega(\\sqrt{n})$ lower bound on the deviation of $(T_i)_{i=1}^K$ from the optimal allocation $(T_i^*)_{i=1}^K$ complements the corresponding $\\mathcal{O} \\left( \\sqrt{n \\log n} \\right)$ upper bound derived in Theorem~1 of (Antos et a. 2008)4 for their GAFS-MAX algorithm. A similar upper bound was also obtained by (Carpentier et al. 2011)5  for their UCB-type algorithm. Thus our lower bound result demonstrates the near-optimality of these two existing algorithms.\nAs a corollary of the above proposition, we can obtain a lower bound on the excess loss of any allocation scheme.\n Corollary. The minimax excess risk for active learning in the case of 2-armed bandit problems satisfies the following: $$ \\inf_{\\mathcal{A}}\\; \\max_{\\mathcal{M}, \\mathcal{N}}\\; \\mathbb{E}\\left[ \\mathcal{L}(T_1, T_2) - \\mathcal{L}(T_1^*, T_2^*) \\right] = \\Omega \\left( n^{-3/2} \\right). $$\n Informally, the proof of the corollary uses the following idea: Since the objective function $\\mathcal{L}$ at the optimal allocation is equal to $\\sigma_i^2/T_i$, the deviation from this due to suboptimal allocation $T_i$ is roughly of the order of $ (\\sigma_i^2/(T_i^*)^2) \\times |T_i - T_i^*| = (\\Sigma^2/(\\sigma_i^2 n^2))\\times |T_i^*-T_i|$. The result then follows by using the $\\Omega(\\sqrt{n})$ lower bound on $\\max_{i=1,2} \\; |T_i - T_i^*|$ from the previous proposition.\n   Note that we only require $d$ to be non-negative, symmetric and satisfy the triangle inequality for the lemma to work. In fact, as noted in (Yu, 1997), even the requirement of triangle inequality can be waived and the following \u0026lsquo;weak\u0026rsquo; triangle inequality suffices: for some $A \\in (0,1)$ and for any $x,y,z \\in \\mathcal{D}$, we have $d(x,y ) + d(y, z) \\geq A d(x,z)$. \u0026#x21a9;\u0026#xfe0e;\n Yu, B. (1997). Assouad, Fano, and Le Cam. In Festschrift for Lucien Le Cam. \u0026#x21a9;\u0026#xfe0e;\n It is only at this point that we use the fact that $d$ satisfies the triangle inequality. \u0026#x21a9;\u0026#xfe0e;\n Antos, A., Grover, V., and Szepesvári, C. \u0026lsquo; Active learning in multi-armed bandits.\u0026rsquo; ALT, 2008 \u0026#x21a9;\u0026#xfe0e;\n Carpentier, A., Lazaric, A., Ghavamzadeh, M., Munos, R., and Auer, P. \u0026lsquo; Upper-confidence-bound algorithms for active learning in multi-armed bandits.\u0026rsquo; ALT, 2011 \u0026#x21a9;\u0026#xfe0e;\n   ","date":1598510179,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598510179,"objectID":"f9201f7c04f732c40beba3ad72a5d540","permalink":"https://sshekhar17.githuo.io/post/le-cam/","publishdate":"2020-08-26T23:36:19-07:00","relpermalink":"/post/le-cam/","section":"post","summary":"Summary: We introduce LeCam\u0026rsquo;s method for obtaining minimax lower bounds for statistical estimation problems, which proceeds by relating the probabililty of error of a binary hypothesis testing problem to the total-variation distance between the two distributions.","tags":[],"title":"Lower Bound for Active Learning in Bandits via Le Cam's method","type":"post"},{"authors":[],"categories":["Computational Biology"],"content":"ASTRAL (Mirarab et al., 2014)1  is a method of reconstructing species trees from a given set of gene trees that have been reconstructed from sequence data. While it was shown to be statistically consistent in (Mirarab et al., 2014) 1 , not much was known about its sample complexity, i.e., the number of gene trees ($m$) required to reconstruct the true species tree with high probability. In (Shekhar et al., 2018)2, we provided a tight characterization of the data requirement (i.e., $m$) of ASTRAL in terms of the number of leaves ($n$) and the length of the shortest branch ($f$).\nMore formally, under the Multispecies Coalescent (MSC) model, for the class of species trees with $n$ leaves and the shortest branch length ($f$) sufficiently small, we obtained the following results:\n  We first showed that $\\mathcal{O} \\left( f^{-2} \\log n \\right)$ gene trees are sufficient for ASTRAL to output the true species tree with high probability. This result follows from the standard argument of applying Hoeffding’s inequality followed by a union bound.\n  Proving that $m= \\Omega\\left ( f^{-2} \\log n \\right)$ is also necessary was more involved. We proceeded in the following steps:\n  We began by proving a weaker result. We showed that with $m \\leq \\mathcal{O} \\left (f^{-2}\\right)$ gene trees, a quartet species tree is wrongly reconstructed by ASTRAL with probability close to 0.5. For obtaining this result, we reduced the error event to the study the deviation of a binomial random variable and then used the Berry-Esseen theorem to approximate this binomial. Having obtained the result for the quartet ($n=4$) case, we extended this result to the general case by constructing a species tree consisting of a triplet joined to the rest of the tree by a long branch.\n  Using the insights from the previous result, we then strengthened it to match the sufficient conditions on $m$. In particular, by allowing an extra $\\log(n)$ factor and using stronger deviation inequalities for binomial random variables, we showed that the error in reconstructing the quartet species tree is at least $1/n^{a}$ for some $a\u0026gt;0$ . Then by considering a tree with $n/3$ triplets joined by long branches, we showed that the reconstruction error can be made arbitrarily close to $1$, for $m \\leq (a/5)f^{-2} \\log n$.\n    These results imply that for ASTRAL to guarantee correct reconstruction with high probability uniformly over the space of all species trees, $\\Theta\\left(f^{-2}\\log n\\right)$ gene trees are both necessary and sufficient.\nReferences   Mirarab, S., Reaz, R., Bayzid, M. S., Zimmermann, T., Swenson, M. S., \u0026amp; Warnow, T. (2014). ASTRAL: genome-scale coalescent-based species tree estimation. Bioinformatics. \u0026#x21a9;\u0026#xfe0e;\n Shekhar, S., Roch, S., \u0026amp; Mirarab, S. (2017). Species tree estimation using ASTRAL: how many genes are enough?. IEEE/ACM transactions on computational biology and bioinformatics. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1592817042,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592817042,"objectID":"58935ec02d0e5dfb8b283181eca716a3","permalink":"https://sshekhar17.githuo.io/project/sample-complexity-of-species-tree-estimation/","publishdate":"2020-06-22T02:10:42-07:00","relpermalink":"/project/sample-complexity-of-species-tree-estimation/","section":"project","summary":"Precise characterization of sample complexity of a popular species tree reconstruction algorithm - ASTRAL","tags":["ASTRAL","Phylogenetics"],"title":"Sample Complexity of Species Tree Estimation","type":"project"},{"authors":[],"categories":["Active Learning","Reinforcement Learning"],"content":"A $K$-armed bandit problem involves designing a sampling strategy to identify the distribution (i.e., arm) with the largest mean. A related problem, called active learning in bandits considers the objective of learning the means of all the $K$ distributions uniformly well in terms of squared error ((Antos et al., 2008)1 and (Carpentier et al., 2011)2. However, in many applications, it is required to learn the entire distribution in terms of some distance metric $D$, and not just the mean. For instance, consider the task of constructing an accurate model of an MDP given an exploration budget. This can be framed as the problem of learning the transition probability vectors corresponding to all state-action pairs uniformly well in $\\ell_1$ distance. The techniques developed in prior work are not applicable to this problem.\nLearning distributions with bandit feedback In (Shekhar et al., 2019)3, we proposed a general sampling scheme for learning multiple distributions uniformly well in terms of several commonly used distance metrics such as $\\ell_2^2$, total variation, $f$-divergence, and separation distance. Our main contributions were:\n  We began by studying an abstract version of this tracking problem, and proposed and analyzed a general optimistic tracking algorithm.\n  Next, we instantiated this algorithm for the specific problem instances arising in the case of the four distance metrics mentioned above, and obtained high probability bounds on the regret.\n  We showed that the allocation performance of our algorithm cannot be improved by deriving matching lower bounds on the allocation performance on any reasonable algorithm.\n  Finally, we showed through empirical evaluation that our proposed scheme works better than uniform sampling, greedy sampling and forced exploration sampling baselines.\n  Learning MDP models An obstacle in applying the above ideas to learn MDP models, i.e., the $S \\times A$ conditional distributions, is that we cannot observe arbitrary state-action trasitions in an MDP. This can be addressed by designing policies which spend an appropriate proportion of the time in different state-action pairs, as proposed in (Tarbouriech \u0026amp; Lazaric, 2019)4. In (Tarbouriech et al. 2020)5, we took this approach and proposed an algorithm and derived sample complexity results of estimating the model of a finite MDP with $\\epsilon$ accuracy. Next, we also proposed a heuristic exploration strategy, based on weighted maximum entropy, which outperforms some baselines in experiments.\nReferences   Antos, A., Grover, V., \u0026amp; Szepesvári, C. (2008). Active learning in multi-armed bandits.ALT link \u0026#x21a9;\u0026#xfe0e;\n Carpentier, A., Lazaric, A., Ghavamzadeh, M., Munos, R., \u0026amp; Auer, P. (2011). Upper-confidence-bound algorithms for active learning in multi-armed bandits. ALT link \u0026#x21a9;\u0026#xfe0e;\n Shekhar, S., Ghavamzadeh, M., \u0026amp; Javidi, T. (2020). Adaptive Sampling for Estimating Multiple Probability Distributions. ICML link \u0026#x21a9;\u0026#xfe0e;\n Tarbouriech, J., \u0026amp; Lazaric, A. (2019). Active exploration in markov decision processes.AISTATS. link \u0026#x21a9;\u0026#xfe0e;\n Tarbouriech, J., Shekhar, S., Pirotta, M., Ghavamzadeh, M., \u0026amp; Lazaric, A. (2020). Active Model Estimation in Markov Decision Processes. UAI. link \u0026#x21a9;\u0026#xfe0e;\n   ","date":1592816971,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592816971,"objectID":"770829768fcede335e22f24338ad3f7f","permalink":"https://sshekhar17.githuo.io/project/adaptive-sampling-for-estimating-probability-distributions/","publishdate":"2020-06-22T02:09:31-07:00","relpermalink":"/project/adaptive-sampling-for-estimating-probability-distributions/","section":"project","summary":"Optimal scheme for allocating samples to learn $K$ distributions uniformly well.","tags":["Adaptive Sampling","Bandits"],"title":"Active Learning in Bandits and MDPs","type":"project"},{"authors":[],"categories":["Active Learning","Learning Theory"],"content":"Classification with abstention refers to the classification problems in which the learner can also abstain from declaring a label, i.e., a \u0026ldquo;don\u0026rsquo;t know\u0026rdquo; option. It models several applications such as medical diagnostic systems, voice assistant devices and content filtering. In (Shekhar et al., 2019), we proposed and analyzed the first active learning algorithm for this problem motivated by the approach used in our GP bandits work (Shekhar and Javidi, 2018). The scheme works for two abstention models: fixed-cost and bounded-rate and is general enough to work under several active learning paradigms (pool-based, stream-based and membership query). The algorithm proposed in (Shekhar et al., 2019) performs better than prior passive methods, both theoretically and in experiments. Furthermore, we also demonstrate the optimality of our algorithm by deriving matching lower bounds on excess risk.\n","date":1592816934,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592816934,"objectID":"46d8a0938cc5d6cf4af7b8bbaa3a2df1","permalink":"https://sshekhar17.githuo.io/project/active-learning-for-classification-with-abstention/","publishdate":"2020-06-22T02:08:54-07:00","relpermalink":"/project/active-learning-for-classification-with-abstention/","section":"project","summary":"A minimax near-optimal active learning algorithm for classification with abstention.","tags":["minmax","abstention","classification"],"title":"Active Learning for Classification With Abstention","type":"project"},{"authors":[],"categories":["Bandits","Optimization"],"content":"Kernelized bandits refers to a non-Bayesian formulation of the problem of optimizing a black-box function $f$ which can only be accessed through a noisy zero-order oracle. Here, instead of assuming that $f$ is a sample from a Gaussian Process (GP), we assume that the function $f$ has bounded norm in the RKHS associated with the positive-definite kernel $K$. Existing algorithms in this area admit an upper bound on the cumulative regret of the form $$ \\mathcal{R}_n = \\tilde{\\mathcal{O}}\\left( \\gamma_n \\sqrt{n} \\right)$$ where $\\gamma_n = \\max_{S} I(y_S; f)$ is the maximum information gain associated with kernel $K$, where the maximum is over subsets $S$ of cardinality $n$.\nRecent work by (Scarlett et al., 2017)1 demonstrated a large gap in the existing upper bounds on $\\mathcal{R}_n$ and algorithm-independent lower bounds for am important family of kernels. Our work2 seeks to address this issue by proposing an novel algorithm for kernelized bandits with uniformly tighter regret bounds.\n The key idea of our work is to augment the global GP surrogate with Local Polynomial (LP) estimators on the elements of an adaptively constructed partition, $\\mathcal{P}_t$ of the input space $\\mathcal{X}$.\n This idea, combined with an embedding result, which imples that the elements of the RKHS associated with the Matern family of kernels can be embedded into certain Holder spaces, allows us to derive uniformly better regret bounds.\n Our main contributions are:\n  We first propose an algorithm, called LP-GP-UCB, which uses LP estimators along with the global GP surrogate to construct tighter UCB for $f$ to guide the query strategy.\n  Under assumptions that $f$ has finite norm in RKHS and in a Holder space, we obtain general regret bounds for our proposed algorithm.\n  For commonly used kernels, we then obtain embedding results which show that for these kernels the Holder assumption follows from the bounded RKHS norm assumption. This allows us to specialize the general regret bounds for several important kernel families such as Squared Exponential, Matern, Rational-Quadratic and Gamma-Exponential kernels.\n  Next, we propose a computationally efficient heuristic, which employs standard regression trees to construct the non-uniform partition of the input space. Experiments with benchmark functions as well as a hyperparameter tuning task demonstrate the benefits of our proposed approach.\n  For more details please refer to the overview slides here and the preprint here.\nReferences   Scarlett, J., Bogunovic, I., \u0026amp; Cevher, V. (2017). Lower bounds on regret for noisy gaussian process bandit optimization. COLT. \u0026#x21a9;\u0026#xfe0e;\n Shekhar, S., \u0026amp; Javidi, T. (2020). Multi-scale Zero Order Optimization of Smooth Functions in an RKHS. preprint \u0026#x21a9;\u0026#xfe0e;\n   ","date":1592816906,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592816906,"objectID":"04ac00327943a44d4e2e9aa8a8a70ef1","permalink":"https://sshekhar17.githuo.io/project/rkhs-function-optimization/","publishdate":"2020-06-22T02:08:26-07:00","relpermalink":"/project/rkhs-function-optimization/","section":"project","summary":"Algorithm with uniformly improved regret bounds + a computationally efficient heuristic with better empirical performance ","tags":["RKHS","Holder Spaces","Hyperparameter Tuning"],"title":"Kernelized Bandits","type":"project"},{"authors":[],"categories":["Optimization","Bandits"],"content":"Several applications, such as hyperparameter tuning, can be formulated as the problem of optimizing a noisy black-box function ($f$) that is expensive to evaluate. In the field of Bayesian Optimization (also referred to as Gaussian Process Bandits), the process of optimization is driven by utilizing a Gaussian Process surrogate model to guide the search for optimum.\nUnder the assumption that the unknown function $f$ is a sample from a zero mean Gaussian Process (GP) with known covariance function $K$, and given a sampling budget $n$, the goal of the agent is to design a sequential strategy to query the black-box function (noisy zeroth order oracle), in order to learn about its maximizer. The performance of an algorithm is measured by its cumulative regret $\\mathcal{R}_n$, defined as $\\mathcal{R}_n=\\sum_{t=1}^n f(x^*) - f(x_t)$.\nExisting results in literature have obtained regret bounds of the form $\\mathcal{R}_n = \\mathcal{O} \\left ( \\sqrt{n \\log n \\gamma_n}\\right)$, where $\\gamma_n$ is the maximum information that can be gained about $f$ from $n$ samples.\nImproved Bounds for Bayesian GP Bandits. Our work in this area was motivated by the following informal idea:\n Since our goal is to learn about a maximizer of $f$, and not necessarily about the whole function, can we identify cases in which the $\\gamma_n$ based bounds are loose.\n Our main contributions were:\n  Following the above intuition, we first constructed two Gaussian Processes for which we showed that the $\\gamma_n$ based bounds were very loose.\n  Next, we proposed an algorithm which constructs a non-uniform partition of the input space and focuses sampling in the near optimal regions. For this algorithm we obtained bounds on $\\mathcal{R}_n$ which were tighter than the existing results. In particular, we obtained the first sublinear regret bounds for the exponential kernel, and strictly better regret bounds for Matern kernels when $D\u0026gt;\\nu-1$, where $D$ is the dimension of the input space, and $\\nu$ is the smoothness parameter of the Matern kernel.\n  We then extended our algorithm to the case of Contextual GP bandits, and obtained improvements over the results of (Krause and Ong, 2011)1.\n  Finally, we also showed that the techniques developed can also be used to propose a Bayesian version of the Zooming algorithm of (Kleinberg et al., 2008)2.\n  Extension to GP Level Set Estimation In some problems the goal is not to learn about the optimizer of $f$, but instead to estimate the $\\lambda$-level set of the function, i.e., the region of the input space where $f$ is above a threshold $\\lambda$. In (Shekhar and Javidi, 2019)3 , we extended the techniques developed in (Shekhar and Javidi, 2018) 4 to propose a GP level set estimation algorithm with improved convergence rates and lower computational complexity than the previous known results of Gotovos et al., 2013 5. In addition, by exploiting the structured nature of the evaluation points of our proposed algorithm, we also obtained tighter bounds on the information gain of our algorithm.\nReferences   Krause, A., \u0026amp; Ong, C. S. (2011). Contextual gaussian process bandit optimization. Neurips \u0026#x21a9;\u0026#xfe0e;\n Kleinberg, R., Slivkins, A., \u0026amp; Upfal, E. (2008). Multi-armed bandits in metric spaces. STOC \u0026#x21a9;\u0026#xfe0e;\n Shekhar, S., \u0026amp; Javidi, T. (2019). Multi-Scale Gaussian Process Level Set Estimation. AISTATS. \u0026#x21a9;\u0026#xfe0e;\n Shekhar, S., \u0026amp; Javidi, T. (2018). Gaussian Process Bandits with Adaptive Discretization. Electronic Journal of Statistics. \u0026#x21a9;\u0026#xfe0e;\n Gotovos, A., Casati, N., Hitz, G., \u0026amp; Krause, A. (2013). Active learning for level set estimation. IJCAI. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1592816572,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592816572,"objectID":"3a0ff2ff85630c6d62c20bc3d3e51dcb","permalink":"https://sshekhar17.githuo.io/project/bayesian-optimization/","publishdate":"2020-06-22T02:02:52-07:00","relpermalink":"/project/bayesian-optimization/","section":"project","summary":"Low complexity algorithms with improved regret bounds","tags":["Bayesian Optimization","GP Bandits","Hyperparameter Tuning"],"title":"Bayesian Optimization","type":"project"},{"authors":["Shubhanshu Shekhar","Tara Javidi"],"categories":null,"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"5367b4a2fdb53d015bcfb70bea9da0c9","permalink":"https://sshekhar17.githuo.io/publication/shekhar-2020-multi/","publishdate":"2020-06-23T21:00:42.589215Z","relpermalink":"/publication/shekhar-2020-multi/","section":"publication","summary":"We aim to optimize a black-box function f : X→ R under the assumption that f is H¨older smooth and has bounded norm in the Reproducing Kernel Hilbert Space (RKHS) associated with a given kernel K. This problem is known to have an agnostic Gaussian Process (GP) bandit interpretation in which an appropriately constructed GP surrogate model with kernel K is used to obtain an upper confidence bound (UCB) algorithm. In this paper, we propose a new algorithm (LP-GP-UCB) where the usual GP surrogate model is augmented with Local Polynomial (LP) estimators of the H¨older smooth function f to construct a multi-scale upper confidence bound guiding the search for the optimizer. We analyze this algorithm and derive high probability bounds on its simple and cumulative regret. We then prove that the elements of many common reproducing kernel Hilbert spaces are H¨older smooth and obtain the corresponding H¨older smoothness parameters, and hence, specialize our regret bounds for several commonly used and practically relevant kernels. When specialized to the Squared Exponential (SE) kernel, LP-GP-UCB matches the optimal performance, while for the case of Mat´ern kernels (Kν)ν0, it results in uniformly tighter regret bounds for all values of the smoothness parameter ν  0. Most notably, for certain ranges of ν, the algorithm achieves near-optimal bounds on simple and cumulative regrets, matching the algorithm-independent lower bounds up to poly-logarithmic factors, and thus closing the large gap between the existing upper and lower bounds for these values of ν. Additionally, our analysis provides the first explicit regret bounds, in terms of the budget n, for the Rational-Quadratic (RQ) and Gamma-Exponential (GE). Finally, experiments with synthetic functions as well as a Convolutional Neural Network hyperparameter tuning task demonstrate the practical benefits of our multi-scale partitioning approach over some existing algorithms numerically.","tags":null,"title":"Multi-Scale Zero-Order Optimization of Smooth Functions in an RKHS","type":"publication"},{"authors":["Shubhanshu Shekhar","Mohammad Ghavamzadeh","Tara Javidi"],"categories":null,"content":"","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"cc9d699414f57bbce73f218f9d214a85","permalink":"https://sshekhar17.githuo.io/publication/shekhar-2019-adaptive/","publishdate":"2020-06-23T21:00:42.588709Z","relpermalink":"/publication/shekhar-2019-adaptive/","section":"publication","summary":"We consider the problem of allocating samples to a finite set of discrete distributions in order to learn them uniformly well in terms of four common distance measures: ℓ22, ℓ1, f-divergence, and separation distance. To present a unified treatment of these distances, we first propose a general optimistic tracking algorithm and analyze its sample allocation performance w.r.t.~an oracle. We then instantiate this algorithm for the four distance measures and derive bounds on the regret of their resulting allocation schemes. We verify our theoretical findings through some experiments. Finally, we show that the techniques developed in the paper can be easily extended to the related setting of minimizing the average error (in terms of the four distances) in learning a set of distributions.","tags":null,"title":"Adaptive Sampling for Estimating Multiple Probability Distributions","type":"publication"},{"authors":["Shubhanshu Shekhar","Mohammad Ghavamzadeh","Tara Javidi"],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"07b0caf28fd0ee1ff8fc977dab6f7d4e","permalink":"https://sshekhar17.githuo.io/publication/shekhar-2019-active/","publishdate":"2020-06-23T21:00:42.588884Z","relpermalink":"/publication/shekhar-2019-active/","section":"publication","summary":"We construct and analyze active learning algorithms for the problem of binary classification with abstention. We consider three abstention settings: fixed-cost and two variants of boundedrate abstention, and for each of them propose an active learning algorithm. All the proposed algorithms can work in the most commonly used active learning models, i.e., membership-query, pool-based, and stream-based sampling. We obtain upper-bounds on the excess risk of our algorithms in a general non-parametric framework, and establish their minimax near-optimality by deriving matching lower-bounds. Since our algorithms rely on the knowledge of some smoothness parameters of the regression function, we then describe a new strategy to adapt to these unknown parameters in a data-driven manner. Since the worst case computational complexity of our proposed algorithms increases exponentially with the dimension of the input space, we conclude the paper with a computationally efficient variant of our algorithm whose computational complexity has a polynomial dependence over a smaller but rich class of learning problems","tags":null,"title":"Active learning for binary classification with abstention","type":"publication"},{"authors":["Jean Tarbouriech","Shubhanshu Shekhar","Matteo Pirotta","Mohammad Ghavamzadeh","Alessandro Lazaric"],"categories":null,"content":"","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"84bf08702be3b847a4ee6e3b95db383c","permalink":"https://sshekhar17.githuo.io/publication/tarbouriech-2020-active/","publishdate":"2020-06-23T21:00:42.589392Z","relpermalink":"/publication/tarbouriech-2020-active/","section":"publication","summary":"We study the problem of efficient exploration in order to learn an accurate model of an environment, modeled as a Markov decision process (MDP). Efficient exploration in this problem requires the agent to identify the regions in which estimating the model is more difficult and then exploit this knowledge to collect more samples there. In this paper, we formalize this problem, introduce the first algorithm to learn an ϵ-accurate estimate of the dynamics, and provide its sample complexity analysis. While this algorithm enjoys strong guarantees in the large-sample regime, it tends to have a poor performance in early stages of exploration. To address this issue, we propose an algorithm that is based on maximum weighted entropy, a heuristic that stems from common sense and our theoretical analysis. The main idea here is cover the entire state-action space with the weight proportional to the noise in the transitions. Using a number of simple domains with heterogeneous noise in their transitions, we show that our heuristic-based algorithm outperforms both our original algorithm and the maximum entropy algorithm in the small sample regime, while achieving similar asymptotic performance as that of the original algorithm.","tags":null,"title":"Active Model Estimation in Markov Decision Processes","type":"publication"},{"authors":["Shubhanshu Shekhar","Tara Javidi"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"e1b17ad190eda52e6e61e14766f6cd07","permalink":"https://sshekhar17.githuo.io/publication/shekhar-2019-multiscale/","publishdate":"2020-06-23T21:00:42.588533Z","relpermalink":"/publication/shekhar-2019-multiscale/","section":"publication","summary":"In this paper, the problem of estimating the level set of a black-box function from noisy and expensive evaluation queries is considered. A new algorithm for this problem in the Bayesian framework with a Gaussian Process (GP) prior is proposed. The proposed algorithm employs a hierarchical sequence of partitions to explore different regions of the search space at varying levels of detail depending upon their proximity to the level set boundary. It is shown that this approach results in the algorithm having a low complexity implementation whose computational cost is significantly smaller than the existing algorithms for higher dimensional search space X. Furthermore, high probability bounds on a measure of discrepancy between the estimated level set and the true level set for the the proposed algorithm are obtained, which are shown to be strictly better than the existing guarantees for a large class of GPs.In the process, a tighter characterization of the information gain of the proposed algorithm is obtained which takes into account the structured nature of the evaluation points. This approach improves upon the existing technique of bounding the information gain with maximum information gain.","tags":null,"title":"Multiscale Gaussian Process Level Set Estimation","type":"publication"},{"authors":["Anusha Lalitha","Shubhanshu Shekhar","Tara Javidi","Farinaz Koushanfar"],"categories":null,"content":"","date":1541030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541030400,"objectID":"ec39289fcfe6d271fa45641cb80fba94","permalink":"https://sshekhar17.githuo.io/publication/lalitha-2018-fully/","publishdate":"2020-06-23T21:00:42.589053Z","relpermalink":"/publication/lalitha-2018-fully/","section":"publication","summary":"We consider the problem of training a machine learning model over a network of users in a fully decentralized framework. The users take a Bayesian-like approach via the introduction of a belief over the model parameter space. We propose a distributed learning algorithm in which users update their belief by aggregate information from their one-hop neighbors to learn a model that best fits the observations over the entire network. In addition, we also obtain sufficient conditions to ensure that the probability of error is small for every user in the network. Finally, we discuss approximations required for applying this algorithm for training Neural Networks.","tags":null,"title":"Fully decentralized federated learning","type":"publication"},{"authors":["Shubhanshu Shekhar","Tara Javidi"," others"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522540800,"objectID":"6c53edebd2fe14a400df650330bc6097","permalink":"https://sshekhar17.githuo.io/publication/shekhar-2018-gaussian/","publishdate":"2020-06-23T21:00:42.588332Z","relpermalink":"/publication/shekhar-2018-gaussian/","section":"publication","summary":"In this paper, the problem of maximizing a black-box function f:X→R is studied in the Bayesian framework with a Gaussian Process prior. In particular, a new algorithm for this problem is proposed, and high probability bounds on its simple and cumulative regret are established. The query point selection rule in most existing methods involves an exhaustive search over an increasingly fine sequence of uniform discretizations of X. The proposed algorithm, in contrast, adaptively refines X which leads to a lower computational complexity, particularly when X is a subset of a high dimensional Euclidean space. In addition to the computational gains, sufficient conditions are identified under which the regret bounds of the new algorithm improve upon the known results. Finally, an extension of the algorithm to the case of contextual bandits is proposed, and high probability bounds on the contextual regret are presented. ","tags":null,"title":"Gaussian process bandits with adaptive discretization","type":"publication"},{"authors":["Shubhanshu Shekhar","Sebastien Roch","Siavash Mirarab"],"categories":null,"content":"","date":1514160000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514160000,"objectID":"7aaf03abdab7b8487a6bd28bccc9d6d7","permalink":"https://sshekhar17.githuo.io/publication/shekhar-2017-species/","publishdate":"2020-06-23T21:00:42.588083Z","relpermalink":"/publication/shekhar-2017-species/","section":"publication","summary":"Species tree reconstruction from genomic data is increasingly performed using methods that account for sources of gene tree discordance such as incomplete lineage sorting. One popular method for reconstructing species trees from unrooted gene tree topologies is ASTRAL. In this paper, we derive theoretical sample complexity results for the number of genes required by ASTRAL to guarantee reconstruction of the correct species tree with high probability. We also validate those theoretical bounds in a simulation study. Our results indicate that ASTRAL requires O(f -2 log n) gene trees to reconstruct the species tree correctly with high probability where n is the number of species and f is the length of the shortest branch in the species tree. Our simulations, some under the anomaly zone, show trends consistent with the theoretical bounds and also provide some practical insights on the conditions where ASTRAL works well.","tags":null,"title":"Species tree estimation using ASTRAL: how many genes are enough?","type":"publication"},{"authors":null,"categories":null,"content":"\r[06/01/20] Our paper Adaptive Sampling for Learning Probability Distributions got accepted at ICML 2020.\n[05/14/20] Our paper Active Model Estimation in Markov Decision Processes got accepted at UAI 2020.\n[04/22/20] Our paper Active Learning for Classification with Abstention got accepted at ISIT 2020. Update: Nomimated for Jack Wolf Student paper award. \n","date":1512115200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512115200,"objectID":"a0812ae5f3c926fea6faf4472cefc8e2","permalink":"https://sshekhar17.githuo.io/news/","publishdate":"2017-12-01T00:00:00-08:00","relpermalink":"/news/","section":"","summary":"\r\nList of news.\r\n","tags":[],"title":"News","type":"page"}]