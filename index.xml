<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>My Webpage</title>
    <link>https://sshekhar17.githuo.io/</link>
      <atom:link href="https://sshekhar17.githuo.io/index.xml" rel="self" type="application/rss+xml" />
    <description>My Webpage</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 26 Aug 2020 23:36:19 -0700</lastBuildDate>
    <image>
      <url>https://sshekhar17.githuo.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>My Webpage</title>
      <link>https://sshekhar17.githuo.io/</link>
    </image>
    
    <item>
      <title>Lower Bound for Active Learning in Bandits via Le Cam&#39;s method</title>
      <link>https://sshekhar17.githuo.io/post/le-cam/</link>
      <pubDate>Wed, 26 Aug 2020 23:36:19 -0700</pubDate>
      <guid>https://sshekhar17.githuo.io/post/le-cam/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Summary:&lt;/strong&gt; &lt;em&gt;&lt;span style=&#34;color:#586199&#34;&gt;
We introduce  LeCam&amp;rsquo;s method for obtaining minimax lower bounds for statistical estimation
problems, which proceeds by relating the probabililty of error of a binary hypothesis testing problem to the total-variation distance between the two distributions. Then, as a novel application,  we use this technique to derive lower bound on the problem of
active learning in bandits which shows the near optimality of the existing algorithms
due to Antos et al. (2008) and Carpentier et al. (2011). &lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Consider the following setting:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Suppose $\mathcal{P}$ denotes a class of probability distributions,&lt;/li&gt;
&lt;li&gt;$(\mathcal{D}, d)$ is a metric space&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;,&lt;/li&gt;
&lt;li&gt;$\theta: \mathcal{P} \mapsto \mathcal{D}$ represents some $\mathcal{D}$-valued functional,&lt;/li&gt;
&lt;li&gt;$\mathcal{D}_1, \mathcal{D}_2$ represent two disjoint and $2\delta$ separated subsets of $\mathcal{D}$, for some $\delta&amp;gt;0$,&lt;/li&gt;
&lt;li&gt;$\mathcal{P}_i = \theta^{-1}(\mathcal{D}_i )$ for $i=1,2$ are non-empty disjoint subsets of $\mathcal{P}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Suppose we are given a sample $X \sim P$ taking values in some set $\mathcal{X}$  for  $P \in \mathcal{P}$, and let $\hat{\theta}: \mathcal{X} \mapsto \mathcal{D}$ denote an estimator of $\theta(P)$. Then for any estimator $\hat{\theta}$ we can obtain the following lower bound on the maximum expected error:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Lemma 1 (Le Cam).&lt;/strong&gt; With the definitions introduced above, we have
$$
\sup_{P \in \mathcal{P}} \mathbb{E}_P \left[ d \left( \hat{\theta}, , \theta(P) \right) \right] \geq
\delta \max_{P_i \in \mathcal{P}_i} \left(1 - d_{TV}\left(P_1, P_2 \right) \right).
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the above display, $d_{TV}$ denotes the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Total_variation_distance_of_probability_measures&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;total variation distance&lt;/a&gt;  between two distributions, defined as $d_{TV}(P, Q) = \sup_{E } P(E) - Q(E)$ where the supremum is over all measurable sets $E$.
The result above implies that the minmax lower bound for a particular estimation problem is
large if there exist two distributions $P_1$ and $P_2$ which (i) are &amp;lsquo;well-separated&amp;rsquo; in terms of the $d-$metric, and
(ii) are statistically &amp;lsquo;close&amp;rsquo; in terms of $d_{TV}(\cdot, \cdot)$. Due to their opposing nature,
obtaining the best lower bound requires finding the right balance between these two requirements.&lt;/p&gt;
&lt;p&gt;The statement of the above result and its proof are based on the statement and proof of Lemma~1 in &lt;span style=&#34;color:blue&#34;&gt;
(Yu, 1997)&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;h4 id=&#34;proof-of-the-lemma&#34;&gt;Proof of the Lemma&lt;/h4&gt;
&lt;p&gt;First, we select arbitrary $P_i \in \mathcal{P}_i$ for $i=1,2$ and lower-bound the supremum over all $P \in \mathcal{P}$ with a simple average over these two distributions.&lt;/p&gt;
&lt;p&gt;$$
\sup_{P \in \mathcal{P}}\;\mathbb{E}_P \left[ d \left( \hat{\theta}, , \theta(P) \right) \right]&lt;br&gt;
\geq \frac{1}{2} \left(  \mathbb{E}_{P_1} \left[ d \left( \hat{\theta}, , \theta(P_1) \right) \right] +
\mathbb{E}_{P_2} \left[ d \left( \hat{\theta}, , \theta(P_2) \right) \right]\right )
$$
Next, with the notation $\theta_i = \theta(P_i)$, we observe&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; the following: for any
$\theta \in \mathcal{D}$ such that $d(\theta, \theta_1)&amp;lt; \delta$, we must have $d(\theta, \theta_2)&amp;gt;\delta$.
Similarly, if $d(\theta, \theta_2)&amp;lt;\delta$ then $d(\theta, \theta_1)&amp;gt;\delta$. Both these results
follow from the fact that $d$ satisfies the triangle inequality and that $d(\theta_1, \theta_2) \geq 2\delta$.
Now,  we define the set $E = \left \{x \in \mathcal{X}\;:\; d\left( \hat{\theta}(x), \theta_1 \right) &amp;lt; d\left( \hat{\theta}(x), \theta_2 \right) \right \}$. Clearly, if $X \in E$, then $d(\hat{\theta}(X), \theta_2) \geq \delta$ and if $X \in E^c$ then
$d(\hat{\theta}, \theta_1) \geq \delta$.  Together, these results imply that $d(\hat{\theta}, \theta_1)
\geq \delta 1_{E^c}$ and $d(\hat{\theta}, \theta_2) \geq \delta 1_E$ where $1_E$ denotes the indicator function
associated with a set $E$.
Thus we have
$$
\begin{aligned}
\sup_{P \in \mathcal{P}}\;\mathbb{E}_P \left[ d \left( \hat{\theta}, \, \theta(P) \right) \right] &amp;amp;
\geq \frac{1}{2} \left(  \mathbb{E}_{P_1} \left[ d \left( \hat{\theta}, \, \theta(P_1) \right) \right] +
\mathbb{E}_{P_2} \left[ d \left( \hat{\theta}, \, \theta(P_2) \right) \right]\right ) \\&lt;br&gt;
&amp;amp; \geq \frac{1}{2} \left( \mathbb{E}_{P_1}[\delta 1_{E^c}  ]  + \mathbb{E}_{P_2}[\delta 1_{E}]\right) \; = \;
\frac{\delta}{2} \left( P_1(E^c) + P_2(E)  \right) \\&lt;br&gt;
&amp;amp; \geq \frac{\delta}{2} \left( 1 - \left(P_1(E) - P_2(E) \right)  \right) \\&lt;br&gt;
&amp;amp; \geq \frac{\delta}{2} \left( 1 - \sup_{E \subset \mathcal{X}}\left(P_1(E) - P_2(E) \right)  \right) \\&lt;br&gt;
&amp;amp; = \frac{\delta}{2} \left( 1 - d_{TV}(P_1, P_2)  \right) \\&lt;br&gt;
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Finally, the result follows by noting the fact that the distributions $P_i \in \mathcal{P}_i$
were chosen arbitrarily, and hence we can take a supremum over all such $P_i$.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;application-to-active-learning-in-bandits&#34;&gt;Application to Active Learning in Bandits.&lt;/h3&gt;
&lt;p&gt;We first describe the problem of active learning in $K$-armed bandits.  A 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Multi-armed_bandit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;multi-armed bandit (MAB)&lt;/a&gt;
problem~(with $K$ arms) consists of $K$ distributions $(P_1, \ldots, P_K)$ which can be individually
sampled by an agent.
In the problem of &lt;em&gt;active learning in bandits&lt;/em&gt;, given a total sampling budget of $n$,
the goal of an agent is to allocate these $n$ samples among
these $K$ distributions, in order to learn their means uniformly well. More specifically, suppose
the agent allocates $T_i \geq 1$ samples to distribution $P_i$, with $\sum_{i=1}^K T_i = n$
and constructs the empirical estimate of the mean of $P_i$, $\hat{\mu}_i(T_i) = \frac{1}{T_i}\sum_{j=1}^{T_i}X_{i,j}$.
Then the goal is to find the allocation $(T_1, \ldots, T_K)$ which solves&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\min_{T_1, \ldots, T_K: \sum_{i=1}^K T_i = n} \;\max_{1 \leq i \leq K} \; \mathbb{E} \left[ |\hat{\mu_i}(T_i) - \mu_i|^2\right] = \left( \max_{1 \leq i \leq K}\frac{ \sigma_i^2}{T_i} \right ) \stackrel{\text{def}}{=}\mathcal{L}(T_1, \ldots, T_K)
\end{aligned}
$$
Allowing, for $T_i$ to take real values, the optimal allocation for this above problem is
given by $T_i^* = \frac{ \sigma_i^2 n}{\Sigma}$ where $\Sigma = \sum_{i=1}^K \sigma_i^2$.
Clearly, the optimal allocation $(T_1^*, \ldots, T_K^*)$ depends on the variance of the
distributions $(P_i)_{i=1}^K$ which are unknown to the agent, and the task&lt;br&gt;
is to design an adaptive sampling strategy which appropriately addresses this 
&lt;a href=&#34;https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0095693&amp;amp;type=printable&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;explore-exploit dilemma&lt;/a&gt;
and  ends up with an allocation $(T_1, \ldots, T_K)$ which is close to the optimal.&lt;/p&gt;
&lt;h4 id=&#34;lower-bound-construction&#34;&gt;Lower bound construction&lt;/h4&gt;
&lt;p&gt;Consider two Bernoulli distributions $U \sim \text{Ber(u)}$ and $V \sim \text{Ber}(v)$ with $1/2 &amp;lt; v &amp;lt; u &amp;lt;1$ and
let $\mathcal{M} = (U, V)$ and $\mathcal{N} = (V, U)$ be two two-armed bandit problems. Suppose an allocation scheme
$\mathcal{A}$ is applied on one of these two problems and results in an allocation $(T_1, T_2)$. Then we have the
following result, which informally says that for $u, v$ close enough, no algorithm can perform well on both the
problems.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Proposition 2.&lt;/strong&gt; We have the following:
$$
\inf_{\mathcal{A}}\; \max_{\mathcal{M}, \mathcal{N}}\; \max_{i =1, 2}\; \mathbb{E}\left[ |T_i - T_i^*| \right] = \Omega (\sqrt{n}).
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;Proof of Proposition 2.&lt;/em&gt; Together with the MAB instance (i.e., either $\mathcal{M}$ or $\mathcal{N}$),
the allocation scheme $\mathcal{A}$ induces a probability distribution on the space of action-observation
sequences $(a_1, X_1, a_2, X_2, \ldots, a_n, X_n)$ where each action $a_t \in \{1,2\}$ and the observations
$X_t$ lie in $\{0, 1\}$ since the distributions $U$ and $V$ are Bernoulli.  We will denote the two resulting
probability distributions by $P_1$ and $P_2$, corresponding to MABs $\mathcal{M}$ and $\mathcal{N}$ respectively.&lt;/p&gt;
&lt;p&gt;We also introduce the notations $T_U^* = \frac{\sigma_U^2 n}{\sigma_U^2 + \sigma_V^2}$ and
$T_V^* = \frac{\sigma_V^2 n}{\sigma_U^2 + \sigma_V^2}$. Then under the MAB $\mathcal{M}$ the
optimal allocations are $(T_1^*, T_2^*) =(T_U^*, T_V^*)$, while they are flipped for the MAB
$\mathcal{N}$. Since we have assumed that $u&amp;gt;v$, we must have $\sigma_U^2 &amp;lt; \sigma_V^2$ which
implies that $T_U^* &amp;lt; n/2 &amp;lt; T_V^*$.&lt;/p&gt;
&lt;p&gt;To apply Lemma~1 to this problem, we introduce the following notations keeping the algorithm $\mathcal{A}$ fixed.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We choose $\mathcal{P}$ to be the set $\{P_1, P_2\}$ where $P_i$ for $i\in \{1,2\}$ were defined earlier. Since there are only two elements in $\mathcal{P}$, we trivially have $\mathcal{P}_i = \{P_i\}$ for $i \in \{1,2\}$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We define $\theta:\mathcal{P} \mapsto \mathbb{R}$ as as the mapping from $P \in \mathcal{P}$ to the corresponding $T_1^* $. That is, $\theta(P_1) = T_U^* $ and $\theta(P_2) = T_V^* $. The metric $d$ is chosen as $d(t_1, t_2) = |t_1 - t_2|$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The estimate $\hat{\theta}$ is the allocation value $T_1$ resulting from the scheme $\mathcal{A}$. Note that since $T_1 + T_2 = n$, we have $|T_1^* - T_1| = |T_2^* - T_2|$. Therefore, we always have $\mathbb{E} [ |T_1-T_1^*| ] = \max_{i = 1,2} \mathbb{E}
[|T_i - T_i^*|]$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Finally, we introduce the notation $\delta = |T_U^* - T_V^*|/2$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Within this setting, a direct application of Lemma~1 gives us
$$
\max_{\mathcal{M}, \mathcal{N}} \; \max_{i=1,2} \; \mathbb{E}[|T_i^*-T_i|] \geq
\delta \left( 1- d_{TV}(P_1, P_2) \right).  \qquad (\star)
$$&lt;/p&gt;
&lt;p&gt;Next, we need to obtain a lower bound on $\delta$ and an upper bound on $d_{TV}(P_1, P_2)$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lower bound on $\delta$:&lt;/strong&gt;   First we note that $\sigma_U^2 = u(1-u)$ and $\sigma_V^2 = v(1-v)$ and $2\delta = n \frac{\sigma_V^2 - \sigma_U^2}{\sigma_V^2 + \sigma_U^2}$. With the notation $v = u-\epsilon$ and the fact that $1/2 &amp;lt; v &amp;lt; u &amp;lt; 1$, we can show that $\delta \geq n (u-1/2)\epsilon$. By choosing $u=3/4$, we get $\delta \geq \frac{n\epsilon}{4}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Upper bound on&lt;/strong&gt; $(d_{TV}(P_1, P_2)):$ To bound $d_{TV}(P_1, P_2)$ we proceed in the following steps:
$$
\begin{aligned}
d_{TV}(P_1, P_2) &amp;amp; \stackrel{(i)}{\leq} \sqrt{\frac{D_{KL}(P_1, P_2)}{2}}
\stackrel{(ii)}{=} \sqrt{ \frac{ \mathbb{E}[T_1]d_{KL}(u,v) + \mathbb{E}[T_2]d_{kl}(v,u)}{2} } \\&lt;br&gt;
&amp;amp; \stackrel{(iii)}{\leq } 4(u-v) \sqrt{ \frac{ \mathbb{E}[T_1] + \mathbb{E}[T_2] }{6} } = \epsilon \sqrt{\frac{8n}{3} }
\end{aligned}
$$
In the above display,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;(i)&lt;/strong&gt; follows from an application of 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Pinsker%27s_inequality&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pinsker&amp;rsquo;s inequality&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;(ii)&lt;/strong&gt; follows from the decomposition lemma for KL-divergence for bandits (see Eq.(5) 
&lt;a href=&#34;https://banditalgs.com/2016/09/28/more-information-theory-and-minimax-lower-bounds/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;), and&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;(iii)&lt;/strong&gt; uses the fact that the bound on KL-divergence for Bernoulli random variables $d_{KL}(u, v) \leq \frac{(u-v)^2}{v(1-v)}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thus plugging these two inequalities back into the inequality $(\star)$, and
choosing $\epsilon = \sqrt{ \frac{3}{32n} }$ gives us
$$
\begin{aligned}
\max_{\mathcal{M}, \mathcal{N}} \; \max_{i=1,2} \; \mathbb{E}[|T_i^*-T_i|] &amp;amp;\geq
\frac{n \epsilon}{4} \left(1 - \epsilon \sqrt{\frac{8n}{3}} \right) \\&lt;br&gt;
&amp;amp; = \frac{1}{32} \sqrt{ \frac{3n}{2} } = \Omega ( \sqrt{n} )
\end{aligned}
$$&lt;/p&gt;
 &lt;div style=&#34;text-align: right&#34;&gt;$\blacksquare$ &lt;/div&gt;
&lt;p&gt;Note that this $\Omega(\sqrt{n})$ lower bound on the deviation of $(T_i)_{i=1}^K$ from
the optimal allocation $(T_i^*)_{i=1}^K$ complements the corresponding $\mathcal{O} \left( \sqrt{n \log n} \right)$
upper bound derived in Theorem~1 of &lt;span style=&#34;color:blue&#34;&gt; (Antos et a. 2008)&lt;/span&gt;&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; for their GAFS-MAX algorithm.
A similar upper bound was also obtained by &lt;span style=&#34;color:blue&#34;&gt; (Carpentier et al. 2011)&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt; &lt;/span&gt; for their UCB-type
algorithm. Thus our lower bound result demonstrates the near-optimality of these two existing algorithms.&lt;/p&gt;
&lt;p&gt;As a corollary of the above proposition, we can obtain a lower bound on the excess
loss of any allocation scheme.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Corollary.&lt;/strong&gt; The minimax excess risk for active learning in the case of 2-armed bandit problems satisfies the following:
$$
\inf_{\mathcal{A}}\; \max_{\mathcal{M}, \mathcal{N}}\;  \mathbb{E}\left[ \mathcal{L}(T_1, T_2) -
\mathcal{L}(T_1^*, T_2^*) \right] = \Omega \left( n^{-3/2} \right).
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Informally, the proof of the corollary uses the following idea: Since the objective
function $\mathcal{L}$ at the optimal allocation is equal to $\sigma_i^2/T_i$, the
deviation from this due to suboptimal allocation $T_i$ is roughly of the order of
$ (\sigma_i^2/(T_i^*)^2) \times |T_i - T_i^*| = (\Sigma^2/(\sigma_i^2 n^2))\times |T_i^*-T_i|$. The
result then follows by using the $\Omega(\sqrt{n})$ lower bound on $\max_{i=1,2} \; |T_i - T_i^*|$
from the previous proposition.&lt;/p&gt;
&lt;hr&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Note that we only require $d$ to be non-negative, symmetric  and satisfy the triangle inequality for the lemma to work. In fact, as noted in &lt;span style=&#34;color:blue&#34;&gt; (Yu, 1997)&lt;/span&gt;, even the requirement of triangle inequality can be waived  and the following &amp;lsquo;weak&amp;rsquo;  triangle inequality suffices: for some $A \in (0,1)$ and for any $x,y,z \in \mathcal{D}$, we have $d(x,y ) + d(y, z) \geq A d(x,z)$. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Yu, B. (1997). 
&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-1-4612-1880-7_29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Assouad, Fano, and Le Cam.&lt;/a&gt; In Festschrift for Lucien Le Cam. &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;It is only at this point that we use the fact that $d$ satisfies the triangle inequality. &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Antos, A., Grover, V., and Szepesvári, C.  &amp;lsquo;
&lt;a href=&#34;https://sites.ualberta.ca/~szepesva/papers/Allocation-TCS09.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Active learning in multi-armed bandits.&lt;/a&gt;&amp;rsquo; &lt;em&gt;ALT, 2008&lt;/em&gt; &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Carpentier, A., Lazaric, A., Ghavamzadeh, M., Munos, R., and  Auer, P. &amp;lsquo;
&lt;a href=&#34;https://arxiv.org/abs/1507.04523&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Upper-confidence-bound algorithms for active learning in multi-armed bandits.&lt;/a&gt;&amp;rsquo; &lt;em&gt;ALT, 2011&lt;/em&gt; &lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Sample Complexity of Species Tree Estimation</title>
      <link>https://sshekhar17.githuo.io/project/sample-complexity-of-species-tree-estimation/</link>
      <pubDate>Mon, 22 Jun 2020 02:10:42 -0700</pubDate>
      <guid>https://sshekhar17.githuo.io/project/sample-complexity-of-species-tree-estimation/</guid>
      <description>&lt;p&gt;ASTRAL &lt;span style=&#34;color:blue&#34;&gt; (Mirarab et al., 2014)&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; &lt;/span&gt; is a method of reconstructing species trees from a given set of gene trees that have been reconstructed from sequence data. While it was shown to be statistically consistent in &lt;span style=&#34;color:blue&#34;&gt; (Mirarab et al., 2014) &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; &lt;/span&gt;, not much was known about its sample complexity, i.e., the number of gene trees ($m$) required to reconstruct the true species tree with high probability. In &lt;span style=&#34;color:blue&#34;&gt;(Shekhar et al., 2018)&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;, we provided a tight characterization of the data requirement (i.e., $m$) of ASTRAL in terms of the number of leaves ($n$) and the length of the shortest branch ($f$).&lt;/p&gt;
&lt;p&gt;More formally, under the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Multispecies_coalescent_process&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Multispecies Coalescent&lt;/a&gt; (MSC) model, for the class of species trees with $n$ leaves and the shortest branch length ($f$) sufficiently small,  we obtained the following results:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We first showed that $\mathcal{O} \left( f^{-2} \log n \right)$ gene trees are sufficient for ASTRAL to output the true species tree with high probability. This result follows from the standard argument of applying Hoeffding’s inequality followed by a union bound.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Proving that $m= \Omega\left ( f^{-2} \log n \right)$ is also necessary was more involved. We proceeded in the following steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We began by proving a weaker result. We showed that with $m \leq \mathcal{O} \left (f^{-2}\right)$ gene trees, a quartet species tree is wrongly reconstructed by ASTRAL with probability close to 0.5. For obtaining this result, we reduced the error event to the study the deviation of a binomial random variable and then used the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Berry-Esseen theorem&lt;/a&gt; to approximate this binomial. Having obtained the result for the quartet ($n=4$) case, we extended this result to the general case by constructing a species tree consisting of a triplet joined to the rest of the tree by a long branch.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Using the insights from the previous result, we then strengthened it to match the sufficient conditions on $m$. In particular, by allowing an extra  $\log(n)$ factor and using stronger deviation inequalities for binomial random variables, we showed that the error in reconstructing the quartet species tree is at least $1/n^{a}$  for some $a&amp;gt;0$ . Then by considering a tree with $n/3$ triplets joined by long branches, we showed that the reconstruction error can be made arbitrarily close to $1$, for $m \leq (a/5)f^{-2} \log n$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These results imply that for ASTRAL to guarantee correct reconstruction with high probability uniformly over the space of all species trees, $\Theta\left(f^{-2}\log n\right)$ gene trees are both necessary and sufficient.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Mirarab, S., Reaz, R., Bayzid, M. S., Zimmermann, T., Swenson, M. S., &amp;amp; Warnow, T. (2014). ASTRAL: genome-scale coalescent-based species tree estimation. &lt;em&gt;Bioinformatics&lt;/em&gt;. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Shekhar, S., Roch, S., &amp;amp; Mirarab, S. (2017). Species tree estimation using ASTRAL: how many genes are enough?. &lt;em&gt;IEEE/ACM transactions on computational biology and bioinformatics&lt;/em&gt;. &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Active Learning in Bandits and MDPs</title>
      <link>https://sshekhar17.githuo.io/project/adaptive-sampling-for-estimating-probability-distributions/</link>
      <pubDate>Mon, 22 Jun 2020 02:09:31 -0700</pubDate>
      <guid>https://sshekhar17.githuo.io/project/adaptive-sampling-for-estimating-probability-distributions/</guid>
      <description>&lt;p&gt;A $K$-armed bandit problem involves designing a sampling strategy to identify the distribution (i.e., arm) with the largest mean. A related problem, called &lt;em&gt;active learning in bandits&lt;/em&gt; considers the objective of learning the means of all the $K$ distributions uniformly well in terms of squared error (&lt;span style=&#34;color:blue&#34;&gt; (Antos et al., 2008)&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; and &lt;span style=&#34;color:blue&#34;&gt; (Carpentier et al., 2011)&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;.
However, in many applications, it is required to learn the entire distribution in terms of some distance metric $D$, and not just the mean.
For instance, consider the task of constructing an accurate model of an MDP given an exploration budget.
This can be framed as the problem of learning the transition probability vectors corresponding to all state-action pairs uniformly well in $\ell_1$ distance. The techniques developed in prior work are not applicable to this problem.&lt;/p&gt;
&lt;h4 id=&#34;learning-distributions-with-bandit-feedback&#34;&gt;Learning distributions with bandit feedback&lt;/h4&gt;
&lt;p&gt;In &lt;span style=&#34;color:blue&#34;&gt; (Shekhar et al., 2019)&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;, we proposed a general sampling scheme for learning multiple distributions uniformly well in terms of several commonly used distance metrics such as $\ell_2^2$, &lt;em&gt;total variation&lt;/em&gt;, &lt;em&gt;$f$-divergence&lt;/em&gt;, and &lt;em&gt;separation distance&lt;/em&gt;.  Our main contributions were:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We began by studying an abstract version of this &lt;em&gt;tracking&lt;/em&gt; problem, and proposed and analyzed a general optimistic tracking algorithm.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Next, we instantiated this algorithm for the specific problem instances arising in the case of the four distance metrics mentioned above, and obtained high probability bounds on the regret.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We showed that the allocation performance of our algorithm cannot be improved by deriving matching lower bounds on the allocation performance on any reasonable algorithm.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Finally, we showed through empirical evaluation that our proposed scheme works better than &lt;em&gt;uniform sampling&lt;/em&gt;, &lt;em&gt;greedy sampling&lt;/em&gt; and &lt;em&gt;forced exploration sampling&lt;/em&gt; baselines.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;learning-mdp-models&#34;&gt;Learning MDP models&lt;/h4&gt;
&lt;p&gt;An obstacle in applying the above ideas to learn MDP models, i.e., the $S \times A$ conditional distributions, is that we cannot  observe arbitrary state-action trasitions in an MDP. This can be addressed by designing policies which spend an appropriate proportion of the time in different state-action pairs, as proposed in &lt;span style=&#34;color:blue&#34;&gt;(Tarbouriech &amp;amp; Lazaric, 2019)&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;. In &lt;span style=&#34;color:blue&#34;&gt;(Tarbouriech et al. 2020)&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;, we took this approach and proposed an algorithm and derived sample complexity results of estimating the model of a finite MDP with $\epsilon$ accuracy. Next, we also proposed a heuristic exploration strategy, based on weighted maximum entropy, which outperforms some baselines in experiments.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Antos, A., Grover, V., &amp;amp; Szepesvári, C. (2008). Active learning in multi-armed bandits.&lt;em&gt;ALT&lt;/em&gt; 
&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-540-87987-9_25&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Carpentier, A., Lazaric, A., Ghavamzadeh, M., Munos, R., &amp;amp; Auer, P. (2011). Upper-confidence-bound algorithms for active learning in multi-armed bandits. &lt;em&gt;ALT&lt;/em&gt; 
&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-642-24412-4_17&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link&lt;/a&gt; &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Shekhar, S., Ghavamzadeh, M., &amp;amp; Javidi, T. (2020). Adaptive Sampling for Estimating Multiple Probability Distributions. &lt;em&gt;ICML&lt;/em&gt; 
&lt;a href=&#34;https://arxiv.org/abs/1910.12406&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link&lt;/a&gt; &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Tarbouriech, J., &amp;amp; Lazaric, A. (2019). Active exploration in markov decision processes.&lt;em&gt;AISTATS&lt;/em&gt;. 
&lt;a href=&#34;https://arxiv.org/abs/1902.11199&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link&lt;/a&gt; &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Tarbouriech, J., Shekhar, S., Pirotta, M., Ghavamzadeh, M., &amp;amp; Lazaric, A. (2020). Active Model Estimation in Markov Decision Processes. &lt;em&gt;UAI&lt;/em&gt;.
&lt;a href=&#34;https://arxiv.org/abs/2003.03297&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link&lt;/a&gt; &lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Active Learning for Classification With Abstention</title>
      <link>https://sshekhar17.githuo.io/project/active-learning-for-classification-with-abstention/</link>
      <pubDate>Mon, 22 Jun 2020 02:08:54 -0700</pubDate>
      <guid>https://sshekhar17.githuo.io/project/active-learning-for-classification-with-abstention/</guid>
      <description>&lt;p&gt;Classification with abstention refers to the classification problems in which the learner can also abstain from declaring a label, i.e., a &lt;em&gt;&amp;ldquo;don&amp;rsquo;t know&amp;rdquo;&lt;/em&gt; option.
It  models  several applications such as  medical diagnostic systems,  voice assistant devices and content filtering.
In 
&lt;a href=&#34;https://arxiv.org/abs/1906.00303&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;span style=&#34;color:blue&#34;&gt;(Shekhar et al., 2019)&lt;/span&gt;&lt;/a&gt;, we proposed and analyzed the first active learning algorithm for this problem  motivated by the  approach used in our GP bandits work 
&lt;a href=&#34;https://projecteuclid.org/euclid.ejs/1543892564&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;span style=&#34;color:blue&#34;&gt;(Shekhar and Javidi, 2018)&lt;/span&gt;&lt;/a&gt;.
The  scheme works for two abstention models: &lt;em&gt;fixed-cost&lt;/em&gt; and &lt;em&gt;bounded-rate&lt;/em&gt; and is general enough to work under several active learning paradigms (pool-based, stream-based and membership query).
The algorithm proposed in 
&lt;a href=&#34;https://arxiv.org/abs/1906.00303&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;span style=&#34;color:blue&#34;&gt;(Shekhar et al., 2019)&lt;/span&gt;&lt;/a&gt; performs better than prior passive methods, both theoretically and in experiments.
Furthermore, we also demonstrate the optimality of our algorithm by deriving &lt;em&gt;matching  lower bounds&lt;/em&gt; on excess risk.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kernelized Bandits</title>
      <link>https://sshekhar17.githuo.io/project/rkhs-function-optimization/</link>
      <pubDate>Mon, 22 Jun 2020 02:08:26 -0700</pubDate>
      <guid>https://sshekhar17.githuo.io/project/rkhs-function-optimization/</guid>
      <description>&lt;p&gt;Kernelized bandits refers to a non-Bayesian formulation of the problem of optimizing a black-box function $f$ which can only be accessed through a noisy zero-order oracle. Here, instead of assuming that $f$ is a sample from a Gaussian Process (GP), we assume that the function $f$ has bounded norm in the RKHS associated with the positive-definite kernel $K$. Existing algorithms in this area admit an upper bound on the cumulative regret of the form $$ \mathcal{R}_n = \tilde{\mathcal{O}}\left( \gamma_n \sqrt{n} \right)$$ where $\gamma_n = \max_{S} I(y_S; f)$ is the &lt;em&gt;maximum information gain&lt;/em&gt; associated with kernel $K$, where the maximum is over subsets $S$ of cardinality $n$.&lt;/p&gt;
&lt;p&gt;Recent work by &lt;span style=&#34;color:blue&#34;&gt;(Scarlett et al., 2017)&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; demonstrated a large gap in the existing upper bounds on $\mathcal{R}_n$ and algorithm-independent lower bounds for am important family of kernels. Our work&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; seeks to address this issue by proposing an novel algorithm for kernelized bandits with uniformly tighter regret bounds.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The key idea of our work is to augment the global GP surrogate with Local Polynomial (LP) estimators on the elements of an adaptively constructed partition, $\mathcal{P}_t$ of the input space $\mathcal{X}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This idea, combined with an embedding result, which imples that the elements of the RKHS associated with the Matern family of kernels can be embedded into certain Holder spaces, allows us to derive uniformly better regret bounds.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Our main contributions are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We first propose an algorithm, called LP-GP-UCB, which uses LP estimators along with the global GP surrogate to construct tighter UCB for $f$ to guide the query strategy.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Under assumptions that $f$ has finite norm in RKHS and in a Holder space, we obtain general regret bounds for our proposed algorithm.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For commonly used kernels, we then obtain embedding results which show that for these kernels the Holder assumption follows from the bounded RKHS norm assumption. This allows us to specialize the general regret bounds for several important kernel families such as Squared Exponential, Matern, Rational-Quadratic and Gamma-Exponential kernels.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Next, we propose a computationally efficient heuristic, which employs standard regression trees to construct the non-uniform partition of the input space. Experiments with benchmark functions as well as a hyperparameter tuning task demonstrate the benefits of our proposed approach.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more details please refer to the overview slides 
&lt;a href=&#34;./slides.pdf&#34;&gt;here&lt;/a&gt; and the preprint 
&lt;a href=&#34;https://arxiv.org/abs/2005.04832&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Scarlett, J., Bogunovic, I., &amp;amp; Cevher, V. (2017). Lower bounds on regret for noisy gaussian process bandit optimization. &lt;em&gt;COLT&lt;/em&gt;. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Shekhar, S., &amp;amp; Javidi, T. (2020). Multi-scale Zero Order Optimization of Smooth Functions in an RKHS. &lt;em&gt;preprint&lt;/em&gt; &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Optimization</title>
      <link>https://sshekhar17.githuo.io/project/bayesian-optimization/</link>
      <pubDate>Mon, 22 Jun 2020 02:02:52 -0700</pubDate>
      <guid>https://sshekhar17.githuo.io/project/bayesian-optimization/</guid>
      <description>&lt;p&gt;Several applications, such as hyperparameter tuning, can be formulated as the problem
of optimizing a noisy black-box function ($f$) that is expensive to evaluate. In the field of 
&lt;a href=&#34;https://distill.pub/2020/bayesian-optimization/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bayesian Optimization&lt;/a&gt; (also referred to as &lt;em&gt;Gaussian Process Bandits&lt;/em&gt;), the process of optimization is driven by utilizing a 
&lt;a href=&#34;&#34;&gt;Gaussian Process&lt;/a&gt; surrogate model to guide the search for optimum.&lt;/p&gt;
&lt;p&gt;Under the assumption that the unknown function $f$ is a sample from a zero mean Gaussian Process (GP) with known covariance function $K$, and given a sampling budget $n$, the goal of the agent is to design a sequential strategy to query
the black-box function (noisy zeroth order oracle), in order to learn about its
maximizer. The performance of an algorithm is measured  by its &lt;strong&gt;cumulative regret&lt;/strong&gt; $\mathcal{R}_n$,
defined as $\mathcal{R}_n=\sum_{t=1}^n f(x^*) - f(x_t)$.&lt;/p&gt;
&lt;p&gt;Existing results in literature have obtained
regret bounds of the form $\mathcal{R}_n = \mathcal{O} \left ( \sqrt{n \log n \gamma_n}\right)$, where $\gamma_n$ is the
maximum information that can be gained about $f$ from $n$ samples.&lt;/p&gt;
&lt;h3 id=&#34;improved-bounds-for--bayesian-gp-banditshttpsprojecteuclidorgeuclidejs1543892564&#34;&gt;
&lt;a href=&#34;https://projecteuclid.org/euclid.ejs/1543892564&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Improved Bounds for  Bayesian GP Bandits&lt;/a&gt;.&lt;/h3&gt;
&lt;p&gt;Our work in this area was motivated by the following informal idea:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Since our goal is to learn about a maximizer of $f$, and not necessarily about the whole function, can we identify cases in which the $\gamma_n$ based bounds are loose.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Our main contributions were:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Following the above intuition, we first constructed two Gaussian Processes for which we showed that the $\gamma_n$ based bounds were very loose.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Next, we proposed an algorithm which constructs a non-uniform partition of the input space and focuses sampling in the near optimal regions. For this algorithm we obtained bounds on $\mathcal{R}_n$ which were tighter than the existing results. In particular, we obtained the &lt;strong&gt;first sublinear regret bounds&lt;/strong&gt; for the exponential kernel, and &lt;strong&gt;strictly better regret bounds for Matern kernels&lt;/strong&gt; when $D&amp;gt;\nu-1$, where $D$ is the dimension of the input space, and $\nu$ is the smoothness parameter of the Matern kernel.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We then extended our algorithm to the case of Contextual GP bandits, and obtained improvements over the results of &lt;span style=&#34;color:blue&#34;&gt;(Krause and Ong, 2011)&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Finally, we also showed that the techniques developed can also be used to propose a Bayesian version of the Zooming algorithm of &lt;span style=&#34;color:blue&#34;&gt; (Kleinberg et al., 2008)&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;extension-to-gp-level-set-estimationhttpproceedingsmlrpressv89shekhar19ahtml&#34;&gt;
&lt;a href=&#34;http://proceedings.mlr.press/v89/shekhar19a.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Extension to GP Level Set Estimation&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;In some problems the goal is not to learn about the optimizer of $f$, but instead to
estimate the $\lambda$-level set of the function, i.e., the region of the input space where $f$ is above a threshold $\lambda$. In &lt;span style=&#34;color:blue&#34;&gt;(Shekhar and Javidi, 2019)&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; , we extended the techniques developed in &lt;span style=&#34;color:blue&#34;&gt;(Shekhar and Javidi, 2018) &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; to propose a GP level set estimation algorithm with &lt;strong&gt;improved convergence rates&lt;/strong&gt; and &lt;strong&gt;lower computational complexity&lt;/strong&gt; than the previous known results of &lt;span style=&#34;color:blue&#34;&gt; Gotovos et al., 2013 &lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;. In addition, by exploiting the structured nature of the evaluation points of our proposed algorithm, we also obtained &lt;strong&gt;tighter bounds on the information gain&lt;/strong&gt; of our algorithm.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Krause, A., &amp;amp; Ong, C. S. (2011). Contextual gaussian process bandit optimization. &lt;em&gt;Neurips&lt;/em&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Kleinberg, R., Slivkins, A., &amp;amp; Upfal, E. (2008). Multi-armed bandits in metric spaces. &lt;em&gt;STOC&lt;/em&gt; &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Shekhar, S., &amp;amp; Javidi, T. (2019). Multi-Scale Gaussian Process Level Set Estimation. &lt;em&gt;AISTATS&lt;/em&gt;. &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Shekhar, S., &amp;amp; Javidi, T. (2018). Gaussian Process Bandits with Adaptive Discretization. &lt;em&gt;Electronic Journal of Statistics&lt;/em&gt;. &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Gotovos, A., Casati, N., Hitz, G., &amp;amp; Krause, A. (2013). Active learning for level set estimation. &lt;em&gt;IJCAI&lt;/em&gt;. &lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Multi-Scale Zero-Order Optimization of Smooth Functions in an RKHS</title>
      <link>https://sshekhar17.githuo.io/publication/shekhar-2020-multi/</link>
      <pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://sshekhar17.githuo.io/publication/shekhar-2020-multi/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Adaptive Sampling for Estimating Multiple Probability Distributions</title>
      <link>https://sshekhar17.githuo.io/publication/shekhar-2019-adaptive/</link>
      <pubDate>Fri, 01 May 2020 00:00:00 +0000</pubDate>
      <guid>https://sshekhar17.githuo.io/publication/shekhar-2019-adaptive/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Active learning for binary classification with abstention</title>
      <link>https://sshekhar17.githuo.io/publication/shekhar-2019-active/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://sshekhar17.githuo.io/publication/shekhar-2019-active/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Active Model Estimation in Markov Decision Processes</title>
      <link>https://sshekhar17.githuo.io/publication/tarbouriech-2020-active/</link>
      <pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://sshekhar17.githuo.io/publication/tarbouriech-2020-active/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multiscale Gaussian Process Level Set Estimation</title>
      <link>https://sshekhar17.githuo.io/publication/shekhar-2019-multiscale/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://sshekhar17.githuo.io/publication/shekhar-2019-multiscale/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Fully decentralized federated learning</title>
      <link>https://sshekhar17.githuo.io/publication/lalitha-2018-fully/</link>
      <pubDate>Thu, 01 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://sshekhar17.githuo.io/publication/lalitha-2018-fully/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Gaussian process bandits with adaptive discretization</title>
      <link>https://sshekhar17.githuo.io/publication/shekhar-2018-gaussian/</link>
      <pubDate>Sun, 01 Apr 2018 00:00:00 +0000</pubDate>
      <guid>https://sshekhar17.githuo.io/publication/shekhar-2018-gaussian/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Species tree estimation using ASTRAL: how many genes are enough?</title>
      <link>https://sshekhar17.githuo.io/publication/shekhar-2017-species/</link>
      <pubDate>Mon, 25 Dec 2017 00:00:00 +0000</pubDate>
      <guid>https://sshekhar17.githuo.io/publication/shekhar-2017-species/</guid>
      <description></description>
    </item>
    
    <item>
      <title>News</title>
      <link>https://sshekhar17.githuo.io/news/</link>
      <pubDate>Fri, 01 Dec 2017 00:00:00 -0800</pubDate>
      <guid>https://sshekhar17.githuo.io/news/</guid>
      <description>








&lt;p&gt;&lt;strong&gt;[06/01/20]&lt;/strong&gt; Our paper &lt;a href=&#34;https://arxiv.org/abs/1910.12406&#34;&gt;Adaptive Sampling for Learning Probability Distributions&lt;/a&gt; got accepted at &lt;a href=&#34;https://icml.cc/Conferences/2020/Dates&#34;&gt;ICML 2020&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;[05/14/20]&lt;/strong&gt; Our paper &lt;a href=&#34;https://arxiv.org/abs/2003.03297&#34;&gt;Active Model Estimation in Markov Decision Processes&lt;/a&gt; got accepted at &lt;a href=&#34;http://www.auai.org/uai2020/&#34;&gt;UAI 2020&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;[04/22/20]&lt;/strong&gt; Our paper &lt;a href=&#34;https://arxiv.org/abs/1906.00303&#34;&gt;Active Learning for Classification with Abstention&lt;/a&gt; got accepted at &lt;a href=&#34;https://2020.ieee-isit.org/&#34;&gt;ISIT 2020&lt;/a&gt;. &lt;span style=&#34;color:red&#34;&gt; Update: &lt;a href=&#34;https://2020.ieee-isit.org/StudentAwardFinalists.asp&#34;&gt;Nomimated for Jack Wolf Student paper award&lt;/a&gt;. &lt;/span&gt;&lt;/p&gt;

</description>
    </item>
    
  </channel>
</rss>
